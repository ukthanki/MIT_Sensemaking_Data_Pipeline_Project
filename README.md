[Go to Back to Home Page](https://ukthanki.github.io/)

# MIT Data Engineering Professional Certification

## Sensemaking Data Pipeline Project

<p align="center">
    <img width="100%" src="https://github.com/ukthanki/MIT_Sensemaking_Data_Pipeline_Project/assets/42117481/80ab1b7e-3d60-455e-978a-70c1ae1662a9">
</p>

Up to this point in the course, we had learned the following topics:
1. NumPy
2. Pandas
3. SQL
4. Linear Regression
5. ETL Fundamentals
6. Flask
7. Java
8. CDC Fundamentals
9. Docker
10. Maven
11. NiFi
12. Hadoop
13. PySpark
14. Airflow
15. Advanced Probability
16. DASK
17. JavaScript

In this project, we used the various skills we have learned to pull unstructured data from the MIT course catalog, clean and analyze the data to determine the word counts throughout all of the course names, and display a visual analysis in a D3 web application. The project was presented in a way that a majority of the base code was already provided to us and we had to fill in various sections with code.

We started in the *assignment.py* file as this was the file that would be used by Airflow. Our fist objective was to define a function called *catalog()* that would be responsible for extracting and storing the unstructured data from a series of URLs into files - one for each course catalog URL. I updated the provided pseudocode to generate the following function:

```python
def catalog():

    def pull(url):
        response = urllib.request.urlopen(url).read()
        data = response.decode('utf-8')
        return data
         
    def store(data,file):
        f = open(file,"w+")
        f.write(data)
        f.close()
        print('wrote file: ' + file)

    urls = ['http://student.mit.edu/catalog/m1a.html',
            'http://student.mit.edu/catalog/m1b.html',...]

    for url in urls:
        index = url.rfind('/') + 1
        data = pull(url)
        file = url[index:]
        store(data,file)

        print('pulled: ' + file)
        print('--- waiting ---')
        time.sleep(15)

```

Having collected the unstructured data in several, separate files, we now had to combine them into a single file with the *combine()* function, as shown below:

```python
def combine():
    with open('combo.txt', 'w') as outfile:
       for file in glob.glob("*.html"):
           with open(file) as infile:
               outfile.write(infile.read())

```

At this stage, we define the *titles()* function which leverages the BeautifulSoup library, allowing us to scrape web pages. This function results in a JSON file that is used downstream. 

```python
def titles():
    from bs4 import BeautifulSoup
    def store_json(data,file):
       with open(file, 'w', encoding='utf-8') as f:
           json.dump(data, f, ensure_ascii=False, indent=4)
           print('wrote file: ' + file)
    #Open and read the large html file generated by combine()
    file = open('combo.txt','r')
    html = file.read()
    #the following replaces new line and carriage return char
    html = html.replace('\n', ' ').replace('\r', '')
    #the following create an html parser
    soup = BeautifulSoup(html, "html.parser")
    results = soup.find_all('h3')
    titles = []

    # tag inner text
    for item in results:
        titles.append(item.text)
    store_json(titles, 'titles.json')

```

Once we have created our consolidated JSON file, we then define our last two functions that cleans the data with *clean()* and counts the words with *count()*, as shown below:

```python
def clean():
    #complete helper function definition below
    def store_json(data,file):
        with open(file, 'w', encoding='utf-8') as f:
            json.dump(data,f,ensure_ascii=False, indent=4)
            print('wrote file: ' + file)
        
    with open('titles.json','r') as file:
        titles = json.load(file)
       # remove punctuation/numbers
        for index, title in enumerate(titles):
           punctuation= '''!()-[]{};:'"\,<>./?@#$%^&*_~1234567890'''
           translationTable= str.maketrans("","",punctuation)
           clean = title.translate(translationTable)
           titles[index] = clean

       # remove one character words
        for index, title in enumerate(titles):
            clean = ' '.join( [word for word in title.split() if len(word)>1] )
            titles[index] = clean

        store_json(titles, 'titles_clean.json')

def count_words():
    from collections import Counter
    def store_json(data,file):    
        with open(file,'w',encoding='utf-8') as f:
            json.dump(data,f,ensure_ascii=False, indent=4)
            print('wrote file: ' + file)

    with open('titles_clean.json','r') as file:
        titles = json.load(file)
        words = []

        # extract words and flatten
        for title in titles:
            words.extend(title.split())

        # count word frequency
        counts = Counter(words)
        store_json(counts, 'words.json')

```

With our functions in place, we defined the Directed Acyclic Graph (DAG) by creating six tasks, with the first being a *pip install* command, and the remaining being the five functions defined earlier. My final code for this step is shown below:

```python
with DAG(
   "assignment",
   start_date=days_ago(1),
   schedule_interval="@daily",catchup=False,
) as dag:
# INSTALL BS4 BY HAND THEN CALL FUNCTION
   # ts are tasks
   t0 = BashOperator(
       task_id='task_zero',
       bash_command='pip install beautifulsoup4',
       retries=2)
   t1 = PythonOperator(
       task_id='task_one',
       depends_on_past=False,
       python_callable=catalog)
   t2 = PythonOperator(
       task_id='task_two',
       depends_on_past=False,
       python_callable=combine)
   t3 = PythonOperator(
       task_id='task_three',
       depends_on_past=False,
       python_callable=titles)
   t4 = PythonOperator(
       task_id='task_four',
       depends_on_past=False,
       python_callable=clean)
   t5 = PythonOperator(
       task_id='task_five',
       depends_on_past=False,
       python_callable=count_words)

   t0 >> t1 >> t2 >> t3 >> t4 >> t5

```

We were now able to transition into the execution of the code which required the *assignment.py* file to be placed in the /dags folder so that Airflow would pick it up and load the tasks defined earlier as a DAG. We then initiated our Airflow container in Docker and navigated to http://localhost:8080/ to access our Airflow session. I then manually ran each task in Airflow, and each task succeeded, as shown below in Figure 1:

| ![image](https://github.com/ukthanki/MIT_Sensemaking_Data_Pipeline_Project/assets/42117481/d825bbf9-ee9b-4eab-b399-5f0e251149aa)| 
|:--:| 
| **Figure 1.** Successful Airflow tasks. |

The data from the JSON file that was generated had to be transferred to a .js file so that we could visualize the data. A snippet of the file is shown below in Figure 2:



| ![image](https://github.com/ukthanki/MIT_Sensemaking_Data_Pipeline_Project/assets/42117481/7475daf4-48e9-4bdb-aa36-b183d4f83807)| 
|:--:| 
| **Figure 2.** Word frequencies in a JavaScript file. |

Finally, we used the code provided to us in the *d3_bubble_chart_example.html* file to display a visualization reflecting the word frequencies using the D3 Library. The more frequent a word was used, the bigger the bubble, as shown below in Figure 3:

| ![Untitled](https://github.com/ukthanki/MIT_Sensemaking_Data_Pipeline_Project/assets/42117481/deb77ce6-7517-428d-9715-ee589c895944)| 
|:--:| 
| **Figure 3.** Word frequencies visualization. |

This project was quite interesting as it allowed us to use Airflow as well as JavaScript, a language I had never used before. The ned result was visually pleasing and illustrated that as data engineers, we may need to deal with both structured and unstructured data.

**You can view the full Project in the Repository.**
