[Go to Back to Home Page](https://ukthanki.github.io/)

# MIT Data Engineering Professional Certification

## Sensemaking Data Pipeline Project

<p align="center">
    <img width="100%" src="https://github.com/ukthanki/MIT_Sensemaking_Data_Pipeline_Project/assets/42117481/80ab1b7e-3d60-455e-978a-70c1ae1662a9">
</p>

Up to this point in the course, we had learned the following topics:
1. NumPy
2. Pandas
3. SQL
4. Linear Regression
5. ETL Fundamentals
6. Flask
7. Java
8. CDC Fundamentals
9. Docker
10. Maven
11. NiFi
12. Hadoop
13. PySpark
14. Airflow
15. Advanced Probability
16. DASK
17. JavaScript

In this project, we used the various skills we have learned to pull unstructured data from the MIT course catalog, clean and analyze the data to determine the word counts throughout all of the course names, and display a visual analysis in a D3 web application. The project was presented in a way that a majority of the base code was already provided to us and we had to fill in various sections with code.

We started in the *assignment.py* file as this was the file that would be used by Airflow. Our fist objective was to define a function called *catalog()* that would be responsible for extracting and storing the unstructured data from a series of URLs into files - one for each course catalog URL. I updated the provided pseudocode to generate the following function:

```python
def catalog():

    def pull(url):
        response = urllib.request.urlopen(url).read()
        data = response.decode('utf-8')
        return data
         
    def store(data,file):
        f = open(file,"w+")
        f.write(data)
        f.close()
        print('wrote file: ' + file)

    urls = ['http://student.mit.edu/catalog/m1a.html',
            'http://student.mit.edu/catalog/m1b.html',...]

    for url in urls:
        index = url.rfind('/') + 1
        data = pull(url)
        file = url[index:]
        store(data,file)

        print('pulled: ' + file)
        print('--- waiting ---')
        time.sleep(15)

```

Having collected the unstructured data in several, separate files, we now had to combine them into a sinlge file with the *combine()* function, as shown below:

```python
def combine():
    with open('combo.txt', 'w') as outfile:
       for file in glob.glob("*.html"):
           with open(file) as infile:
               outfile.write(infile.read())

```

At this stage, we define the *titles()* function which leverages the BeautifulSoup library, allowing us to scrape web pages. This function results in a JSON file that is used downstream. 

```python
def titles():
    from bs4 import BeautifulSoup
    def store_json(data,file):
       with open(file, 'w', encoding='utf-8') as f:
           json.dump(data, f, ensure_ascii=False, indent=4)
           print('wrote file: ' + file)
    #Open and read the large html file generated by combine()
    file = open('combo.txt','r')
    html = file.read()
    #the following replaces new line and carriage return char
    html = html.replace('\n', ' ').replace('\r', '')
    #the following create an html parser
    soup = BeautifulSoup(html, "html.parser")
    results = soup.find_all('h3')
    titles = []

    # tag inner text
    for item in results:
        titles.append(item.text)
    store_json(titles, 'titles.json')

```

Once we have created our consolidated JSON file, we then define our last two functions that cleans the data with *clean()* and counts the words with *count()*, as shown below:

```python
def clean():
    #complete helper function definition below
    def store_json(data,file):
        with open(file, 'w', encoding='utf-8') as f:
            json.dump(data,f,ensure_ascii=False, indent=4)
            print('wrote file: ' + file)
        
    with open('titles.json','r') as file:
        titles = json.load(file)
       # remove punctuation/numbers
        for index, title in enumerate(titles):
           punctuation= '''!()-[]{};:'"\,<>./?@#$%^&*_~1234567890'''
           translationTable= str.maketrans("","",punctuation)
           clean = title.translate(translationTable)
           titles[index] = clean

       # remove one character words
        for index, title in enumerate(titles):
            clean = ' '.join( [word for word in title.split() if len(word)>1] )
            titles[index] = clean

        store_json(titles, 'titles_clean.json')

def count_words():
    from collections import Counter
    def store_json(data,file):    
        with open(file,'w',encoding='utf-8') as f:
            json.dump(data,f,ensure_ascii=False, indent=4)
            print('wrote file: ' + file)

    with open('titles_clean.json','r') as file:
        titles = json.load(file)
        words = []

        # extract words and flatten
        for title in titles:
            words.extend(title.split())

        # count word frequency
        counts = Counter(words)
        store_json(counts, 'words.json')

```

As a result, the data was effectively loaded into the database table, as shown below in Figure 1:

| ![download](https://github.com/ukthanki/MIT_MRTS_ETL/assets/42117481/ae5ff829-5165-419a-aa87-0663c492c8b0)| 
|:--:| 
| **Figure 1.** MRTS data loaded into the *mrts* table in MySQL. |

I was then able to execute various SELECT statements through Python to visualize the data to gain various insights. For example, by executing the following code below, I was able to plot the data using Matplotlib, as shown below in Figure 2:

```python
# (5) Retail and food services sales, total Yearly Trend
query5 = """
SELECT SUM(`value`), YEAR(period) FROM mrts WHERE kind_of_business = 'Retail and food services sales, total'
GROUP BY 2 ORDER BY period
"""
MyCursor.execute(query5)
month = []
sales = []
for row in MyCursor.fetchall():
    sales.append(row[0])
    month.append(row[1])
    
plt.plot(month, sales)
plt.title("Retail and food services sales, total - Yearly")
plt.xlabel("Year")
plt.ylabel("Sales (USD, Million)")
plt.show()
```

| ![download](https://github.com/ukthanki/MIT_MRTS_ETL/assets/42117481/0ee68c9d-b29c-4251-b37e-93067cfae930)| 
|:--:| 
| **Figure 2.** Sales vs. Year for Retail and Food Services. |

This project was quite insightful because it focused heavily on ETL and how it may be done programmatically as opposed to manually. I could have produced the same plots by performing all steps in Python only, but by loading the data in MySQL, it became available to a wider audience for querying and analysis; this represents a real-world situation as a result because data must be accessible easily by multiple entities.

**You can view the full Project in the "module_8.py" and "Module 8_Umang_Thanki.ipynb" files in the Repository.**

[Go to Repo](https://github.com/ukthanki/MIT_Sensemaking_Data_Pipeline_Project)

In this project, we extract unstructured date from the MIT Course Catalog, clean and process the data, and visualize it in a D3 Web Application.
